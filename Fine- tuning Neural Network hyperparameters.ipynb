{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated hyperparameter tunning\n",
    "To run this code you should have scikit-learn verstion 0.21.2\n",
    "FIrst we import basic mobules we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "assert sys.version_info >= (3,5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# To plot\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# for random search cv\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.21.2.\n"
     ]
    }
   ],
   "source": [
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "# it should be 0.21.2 because the hyper param tuning will not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we import the data, the first comments is an example of how we import the data on mac the second on windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>543.659973</td>\n",
       "      <td>545.809998</td>\n",
       "      <td>539.760010</td>\n",
       "      <td>543.299988</td>\n",
       "      <td>543.299988</td>\n",
       "      <td>1538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-07-02</td>\n",
       "      <td>544.739990</td>\n",
       "      <td>548.000000</td>\n",
       "      <td>543.570007</td>\n",
       "      <td>547.340027</td>\n",
       "      <td>547.340027</td>\n",
       "      <td>1406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-07-06</td>\n",
       "      <td>542.250000</td>\n",
       "      <td>548.580017</td>\n",
       "      <td>542.099976</td>\n",
       "      <td>545.619995</td>\n",
       "      <td>545.619995</td>\n",
       "      <td>1280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-07-07</td>\n",
       "      <td>547.429993</td>\n",
       "      <td>551.000000</td>\n",
       "      <td>539.849976</td>\n",
       "      <td>550.030029</td>\n",
       "      <td>550.030029</td>\n",
       "      <td>1678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-07-08</td>\n",
       "      <td>545.599976</td>\n",
       "      <td>548.280029</td>\n",
       "      <td>541.200012</td>\n",
       "      <td>541.700012</td>\n",
       "      <td>541.700012</td>\n",
       "      <td>1383100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2015-07-01  543.659973  545.809998  539.760010  543.299988  543.299988   \n",
       "1  2015-07-02  544.739990  548.000000  543.570007  547.340027  547.340027   \n",
       "2  2015-07-06  542.250000  548.580017  542.099976  545.619995  545.619995   \n",
       "3  2015-07-07  547.429993  551.000000  539.849976  550.030029  550.030029   \n",
       "4  2015-07-08  545.599976  548.280029  541.200012  541.700012  541.700012   \n",
       "\n",
       "    Volume  \n",
       "0  1538000  \n",
       "1  1406200  \n",
       "2  1280700  \n",
       "3  1678900  \n",
       "4  1383100  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"/Users/vasileioskilis/Desktop/projectA1988/dataset/GOOGL5yearsd.csv\"\n",
    "# \"C:\\\\Users\\\\KILDE\\\\Desktop\\\\projectA\\\\dataset\\\\GOOGL5yearsd.csv\"\n",
    "stock=pd.read_csv(\"C:\\\\Users\\\\KILDE\\\\Desktop\\\\projectA\\\\dataset\\\\GOOGL5yearsd.csv\")\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We create a panada series, with Date as an index to the closing price and scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2015-07-01    543.299988\n",
       "2015-07-02    547.340027\n",
       "2015-07-06    545.619995\n",
       "2015-07-07    550.030029\n",
       "2015-07-08    541.700012\n",
       "Name: Close, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_price = stock['Close']\n",
    "close_price.index = stock['Date']\n",
    "close_price.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_price = stock.Close.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_price = np.reshape(close_price,(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00162736],\n",
       "       [0.00573656],\n",
       "       [0.00398709],\n",
       "       ...,\n",
       "       [0.83489126],\n",
       "       [0.87011407],\n",
       "       [0.8913515 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_normalised = scaler.fit_transform(close_price)\n",
    "data_normalised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We split the dataset into 80% train, 10% validation and 10% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(data_normalised) * 0.80)\n",
    "half_rest = int((len(data_normalised) - train_size)/2)\n",
    "\n",
    "train_set = data_normalised[0:train_size,:]\n",
    "val_set = data_normalised[train_size:train_size+half_rest,:]\n",
    "test_set = data_normalised[train_size+half_rest:len(data_normalised),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method that creates sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWindows(dataset, window):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    \n",
    "    for i in range(len(dataset)-window-1):\n",
    "        X_train.append(dataset[i:(i+window),0])\n",
    "        Y_train.append(dataset[i+window,0])\n",
    "    return np.array(X_train),np.array(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the windows and reshape the data so we can feed them to the LSTM\n",
    "\n",
    "Input shape for an lstm (batch_size, time_steps, seq_len), window size 20 ,single step prediction the last element of the window is the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = createWindows(train_set,20)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, Y_val = createWindows(val_set,20)\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = createWindows(test_set,20)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we do the walk forward validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "dev_size=0.1 \n",
    "n_splits=int((1//dev_size)-1)\n",
    "tscv=TimeSeriesSplit(n_splits=n_splits) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the method that created our LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLSTM(n_hidden=2, n_neurons=200, learning_rate=0.001, input_shape=X_train.shape[-2:]):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(tf.keras.layers.LSTM(n_neurons, activation=\"relu\",return_sequences=True))\n",
    "        model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we use a Keras regressor wrapper on our method that created the LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model= tf.keras.wrappers.scikit_learn.KerasRegressor(createLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 986 samples, validate on 105 samples\n",
      "Epoch 1/15\n",
      "986/986 [==============================] - 4s 4ms/sample - loss: 0.0734 - val_loss: 0.0023\n",
      "Epoch 2/15\n",
      "986/986 [==============================] - 0s 183us/sample - loss: 0.0038 - val_loss: 9.8294e-04\n",
      "Epoch 3/15\n",
      "986/986 [==============================] - 0s 174us/sample - loss: 0.0028 - val_loss: 0.0015\n",
      "Epoch 4/15\n",
      "986/986 [==============================] - 0s 177us/sample - loss: 0.0026 - val_loss: 6.6029e-04\n",
      "Epoch 5/15\n",
      "986/986 [==============================] - 0s 184us/sample - loss: 0.0027 - val_loss: 6.7327e-04\n",
      "Epoch 6/15\n",
      "986/986 [==============================] - 0s 182us/sample - loss: 0.0025 - val_loss: 5.7515e-04\n",
      "Epoch 7/15\n",
      "986/986 [==============================] - 0s 184us/sample - loss: 0.0021 - val_loss: 6.1725e-04\n",
      "Epoch 8/15\n",
      "986/986 [==============================] - 0s 178us/sample - loss: 0.0025 - val_loss: 8.7115e-04\n",
      "Epoch 9/15\n",
      "986/986 [==============================] - 0s 197us/sample - loss: 0.0023 - val_loss: 4.6872e-04\n",
      "Epoch 10/15\n",
      "986/986 [==============================] - 0s 182us/sample - loss: 0.0022 - val_loss: 0.0017\n",
      "Epoch 11/15\n",
      "986/986 [==============================] - 0s 183us/sample - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 12/15\n",
      "986/986 [==============================] - 0s 178us/sample - loss: 0.0020 - val_loss: 4.9762e-04\n",
      "Epoch 13/15\n",
      "986/986 [==============================] - 0s 184us/sample - loss: 0.0019 - val_loss: 5.0256e-04\n",
      "Epoch 14/15\n",
      "986/986 [==============================] - 0s 183us/sample - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 15/15\n",
      "986/986 [==============================] - 0s 192us/sample - loss: 0.0019 - val_loss: 0.0018\n",
      "105/1 [==============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 67us/sample - loss: 0.0041\n"
     ]
    }
   ],
   "source": [
    "test_model.fit(X_train, Y_train, epochs=15,\n",
    "validation_data=(X_val, Y_val),\n",
    "callbacks=[tf.keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = test_model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we do the randomized search\n",
    "The cross validation methos (cv) is the walk forward validation. The scoring method is mean squared error and the parameters are the ones we wanted to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114 samples\n",
      "114/114 [==============================] - 3s 26ms/sample - loss: 0.0243\n",
      "Train on 223 samples\n",
      "223/223 [==============================] - 3s 15ms/sample - loss: 0.0189\n",
      "Train on 332 samples\n",
      "332/332 [==============================] - 3s 10ms/sample - loss: 0.0173\n",
      "Train on 441 samples\n",
      "441/441 [==============================] - 3s 7ms/sample - loss: 0.0194\n",
      "Train on 550 samples\n",
      "550/550 [==============================] - 4s 7ms/sample - loss: 0.0217\n",
      "Train on 659 samples\n",
      "659/659 [==============================] - 3s 5ms/sample - loss: 0.0312\n",
      "Train on 768 samples\n",
      "768/768 [==============================] - 3s 4ms/sample - loss: 0.0347\n",
      "Train on 877 samples\n",
      "877/877 [==============================] - 3s 4ms/sample - loss: 0.0284\n",
      "Train on 114 samples\n",
      "114/114 [==============================] - 5s 46ms/sample - loss: 0.0294\n",
      "Train on 223 samples\n",
      "223/223 [==============================] - 5s 24ms/sample - loss: 0.0304\n",
      "Train on 332 samples\n",
      "332/332 [==============================] - 5s 15ms/sample - loss: 0.0355\n",
      "Train on 441 samples\n",
      "441/441 [==============================] - 5s 12ms/sample - loss: 0.0381\n",
      "Train on 550 samples\n",
      "550/550 [==============================] - 5s 9ms/sample - loss: 0.0507\n",
      "Train on 659 samples\n",
      "659/659 [==============================] - 5s 8ms/sample - loss: 0.0791\n",
      "Train on 768 samples\n",
      "768/768 [==============================] - 5s 6ms/sample - loss: 0.0820\n",
      "Train on 877 samples\n",
      "877/877 [==============================] - 5s 6ms/sample - loss: 0.0926\n",
      "Train on 114 samples\n",
      "114/114 [==============================] - 1s 13ms/sample - loss: 0.0171\n",
      "Train on 223 samples\n",
      "223/223 [==============================] - 2s 9ms/sample - loss: 0.0120\n",
      "Train on 332 samples\n",
      "332/332 [==============================] - 1s 4ms/sample - loss: 0.0120\n",
      "Train on 441 samples\n",
      "441/441 [==============================] - 2s 4ms/sample - loss: 0.0124\n",
      "Train on 550 samples\n",
      "550/550 [==============================] - 1s 3ms/sample - loss: 0.0169\n",
      "Train on 659 samples\n",
      "659/659 [==============================] - 1s 2ms/sample - loss: 0.0165\n",
      "Train on 768 samples\n",
      "768/768 [==============================] - 1s 2ms/sample - loss: 0.0166\n",
      "Train on 877 samples\n",
      "877/877 [==============================] - 1s 2ms/sample - loss: 0.0237\n",
      "Train on 114 samples\n",
      "114/114 [==============================] - 3s 28ms/sample - loss: 0.0290\n",
      "Train on 223 samples\n",
      "223/223 [==============================] - 3s 15ms/sample - loss: 0.0299\n",
      "Train on 332 samples\n",
      "332/332 [==============================] - 3s 8ms/sample - loss: 0.0308\n",
      "Train on 441 samples\n",
      "441/441 [==============================] - 3s 6ms/sample - loss: 0.0363\n",
      "Train on 550 samples\n",
      "550/550 [==============================] - 3s 6ms/sample - loss: 0.0500\n",
      "Train on 659 samples\n",
      "659/659 [==============================] - 4s 5ms/sample - loss: 0.0678\n",
      "Train on 768 samples\n",
      "768/768 [==============================] - 3s 4ms/sample - loss: 0.0756\n",
      "Train on 877 samples\n",
      "877/877 [==============================] - 3s 3ms/sample - loss: 0.0756\n",
      "Train on 114 samples\n",
      "114/114 [==============================] - 3s 24ms/sample - loss: 0.0260\n",
      "Train on 223 samples\n",
      "223/223 [==============================] - 3s 14ms/sample - loss: 0.0223\n",
      "Train on 332 samples\n",
      "332/332 [==============================] - 3s 10ms/sample - loss: 0.0178\n",
      "Train on 441 samples\n",
      "441/441 [==============================] - 3s 7ms/sample - loss: 0.0194\n",
      "Train on 550 samples\n",
      "550/550 [==============================] - 3s 5ms/sample - loss: 0.0246\n",
      "Train on 659 samples\n",
      "659/659 [==============================] - 3s 5ms/sample - loss: 0.0311\n",
      "Train on 768 samples\n",
      "768/768 [==============================] - 3s 3ms/sample - loss: 0.0363\n",
      "Train on 877 samples\n",
      "877/877 [==============================] - 3s 3ms/sample - loss: 0.0372\n",
      "Train on 114 samples\n",
      "114/114 [==============================] - 2s 17ms/sample - loss: 0.0285\n",
      "Train on 223 samples\n",
      "223/223 [==============================] - 1s 6ms/sample - loss: 0.0255\n",
      "Train on 332 samples\n",
      "332/332 [==============================] - 1s 4ms/sample - loss: 0.0261\n",
      "Train on 441 samples\n",
      "441/441 [==============================] - 2s 4ms/sample - loss: 0.0209\n",
      "Train on 550 samples\n",
      "550/550 [==============================] - 1s 2ms/sample - loss: 0.0310\n",
      "Train on 659 samples\n",
      "659/659 [==============================] - 1s 2ms/sample - loss: 0.0477\n",
      "Train on 768 samples\n",
      "768/768 [==============================] - 2s 3ms/sample - loss: 0.0484\n",
      "Train on 877 samples\n",
      "877/877 [==============================] - 1s 2ms/sample - loss: 0.0600\n",
      "Train on 114 samples\n",
      "114/114 [==============================] - 5s 44ms/sample - loss: 0.0288\n",
      "Train on 223 samples\n",
      "223/223 [==============================] - 5s 24ms/sample - loss: 0.0273\n",
      "Train on 332 samples\n",
      "332/332 [==============================] - 5s 16ms/sample - loss: 0.0290\n",
      "Train on 441 samples\n",
      "441/441 [==============================] - 5s 11ms/sample - loss: 0.0271\n",
      "Train on 550 samples\n",
      "550/550 [==============================] - 6s 11ms/sample - loss: 0.0356\n",
      "Train on 659 samples\n",
      "659/659 [==============================] - 5s 8ms/sample - loss: 0.0457\n",
      "Train on 768 samples\n",
      "768/768 [==============================] - 5s 7ms/sample - loss: 0.0542\n",
      "Train on 877 samples\n",
      "877/877 [==============================] - 5s 6ms/sample - loss: 0.0605\n",
      "Train on 114 samples\n",
      "114/114 [==============================] - 1s 12ms/sample - loss: 0.0169\n",
      "Train on 223 samples\n",
      "223/223 [==============================] - 1s 6ms/sample - loss: 0.0271\n",
      "Train on 332 samples\n",
      "332/332 [==============================] - 2s 7ms/sample - loss: 0.0266\n",
      "Train on 441 samples\n",
      "441/441 [==============================] - 1s 3ms/sample - loss: 0.0237\n",
      "Train on 550 samples\n",
      "550/550 [==============================] - 1s 3ms/sample - loss: 0.0133\n",
      "Train on 659 samples\n",
      "659/659 [==============================] - 2s 3ms/sample - loss: 0.0363\n",
      "Train on 768 samples\n",
      "768/768 [==============================] - 1s 2ms/sample - loss: 0.0453\n",
      "Train on 877 samples\n",
      "877/877 [==============================] - 1s 2ms/sample - loss: 0.0313\n",
      "Train on 114 samples\n",
      "114/114 [==============================] - 1s 12ms/sample - loss: 0.0264\n",
      "Train on 223 samples\n",
      "223/223 [==============================] - 2s 10ms/sample - loss: 0.0257\n",
      "Train on 332 samples\n",
      "332/332 [==============================] - 1s 4ms/sample - loss: 0.0191\n",
      "Train on 441 samples\n",
      "441/441 [==============================] - 1s 3ms/sample - loss: 0.0317\n",
      "Train on 550 samples\n",
      "550/550 [==============================] - 1s 3ms/sample - loss: 0.0181\n",
      "Train on 659 samples\n",
      "659/659 [==============================] - 2s 3ms/sample - loss: 0.0295\n",
      "Train on 768 samples\n",
      "768/768 [==============================] - 1s 2ms/sample - loss: 0.0368\n",
      "Train on 877 samples\n",
      "877/877 [==============================] - 1s 2ms/sample - loss: 0.0462\n",
      "Train on 114 samples\n",
      "114/114 [==============================] - 5s 48ms/sample - loss: 0.0287\n",
      "Train on 223 samples\n",
      "223/223 [==============================] - 5s 24ms/sample - loss: 0.0272\n",
      "Train on 332 samples\n",
      "332/332 [==============================] - 5s 14ms/sample - loss: 0.0285\n",
      "Train on 441 samples\n",
      "441/441 [==============================] - 5s 12ms/sample - loss: 0.0250\n",
      "Train on 550 samples\n",
      "550/550 [==============================] - 5s 10ms/sample - loss: 0.0365\n",
      "Train on 659 samples\n",
      "659/659 [==============================] - 5s 7ms/sample - loss: 0.0410\n",
      "Train on 768 samples\n",
      "768/768 [==============================] - 5s 7ms/sample - loss: 0.0538\n",
      "Train on 877 samples\n",
      "877/877 [==============================] - 5s 5ms/sample - loss: 0.0563\n",
      "Train on 986 samples\n",
      "986/986 [==============================] - 2s 2ms/sample - loss: 0.0220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=TimeSeriesSplit(max_train_size=None, n_splits=8),\n",
       "                   error_score=nan,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001F1BA5DE688>,\n",
       "                   iid='warn', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'learning_rate': [0.001, 0.002, 0.003],\n",
       "                                        'n_hidden': [1, 2, 3],\n",
       "                                        'n_neurons': array([ 30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  4...\n",
       "       186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
       "       199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
       "       212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
       "       225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249])},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=False, scoring='neg_mean_squared_error',\n",
       "                   verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "\"n_hidden\": [1,2,3],\n",
    "\"n_neurons\": np.arange(30,250),\n",
    "\"learning_rate\": [0.001,0.002,0.003],\n",
    "}\n",
    "searchCV = RandomizedSearchCV(test_model,parameters, n_iter=10,cv=tscv,scoring='neg_mean_squared_error',error_score=np.nan)\n",
    "searchCV.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we best model and the bast score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neurons': 241, 'n_hidden': 1, 'learning_rate': 0.003}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00515785121085937"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchCV.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# websites used to create this program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
